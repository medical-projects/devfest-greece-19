{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "@author: Eleftherios Trivizakis\n",
    "@github: github.com/trivizakis\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the basic libraries for the deep learning component.\n",
    "We will utilize a pre-trained DenseNet201 as an \"off-the-shelf\" feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K#K.clear_session()\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import scikit-learn libraries for the classification component of\n",
    "this totaly unscientific experiment ;)\n",
    "We will use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\"StratifiedShuffleSplit\" for a random hold-out split strategy (training &  testing set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\"scale\" for raw feature scaling with values between -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\"Support Vector Machine\" as a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"DataConverter\" is a custom library for converting *.png images to *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DataConverter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some terminal warnings for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"dataset\\\\persona\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Initialize the Input layer. Since we do not use the fully-connected part of the network\n",
    "we could append any input size fits our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Get the ImageNet acquired weights for DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Freeze the convolutional layers, useful when you apply fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Initialize the deep feature extraction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "input_tensor = Input(input_shape) \n",
    "\n",
    "pretrained_model = VGG19(input_tensor=input_tensor, include_top = False, weights = \"imagenet\", pooling = 'avg')\n",
    "\n",
    "# i.e. freeze all convolutional layers\n",
    "#for layer in pretrained_model.layers:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "model = Model(inputs = pretrained_model.input,\n",
    "                     outputs = pretrained_model.output)\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parameters dictionary for\n",
    "converting the png images to npy and then\n",
    "import them as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy files generated.\n"
     ]
    }
   ],
   "source": [
    "hypes={}\n",
    "hypes[\"dataset_dir\"]=ds_path\n",
    "hypes[\"input_shape\"]=input_shape\n",
    "hypes[\"image_normalization\"]=\"-11\"\n",
    "\n",
    "my_file = Path(hypes[\"dataset_dir\"]+\"pids.npy\")\n",
    "if not my_file.is_file():\n",
    "    DataConverter(hypes).convert_png_to_npy()\n",
    "    \n",
    "print(\"npy files generated.\")\n",
    "pids = np.load(ds_path+\"pids.npy\")\n",
    "labels = np.load(ds_path+\"labels.npy\")\n",
    "\n",
    "class_names = [\"aigis\",\"morgana\",\"takamaki\",\"teddie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of the deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_features_raw=[]\n",
    "for pid in pids:\n",
    "    img = np.load(ds_path+pid+\".npy\")\n",
    "    img = np.expand_dims(img, axis=0)    \n",
    "    \n",
    "    deep_features_raw.append(model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape feature maps to a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_features=np.array(deep_features_raw).reshape(-1,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature vector normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep features extracted and scaled.\n"
     ]
    }
   ],
   "source": [
    "deep_features = scale(deep_features,with_mean=True,with_std=True)\n",
    "print(\"Deep features extracted and scaled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Hold-out Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Fitting the classier on the training set using a svm with linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Calculate performance on testing set for each random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance=[]\n",
    "n_splits=5\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, train_size=0.8)\n",
    "for training_indeces, testing_indeces in sss.split(pids,labels):\n",
    "    X_train, X_test = deep_features[training_indeces],deep_features[testing_indeces]\n",
    "    y_train, y_test = labels[training_indeces], labels[testing_indeces]\n",
    "    clf = svm.SVC(kernel='linear', gamma='scale', probability=True)#poly, linear, rbf\n",
    "    clf.fit(X_train, y_train)\n",
    "    performance.append(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 96.92307692307693%\n"
     ]
    }
   ],
   "source": [
    "scores = np.stack(performance)\n",
    "print(\"Mean Acc: \"+str(scores.mean()*100)+\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
