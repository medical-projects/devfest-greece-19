{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "@author: Eleftherios Trivizakis\n",
    "@github: github.com/trivizakis\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom libraries for data handling & deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from dataset import DataConverter\n",
    "from data_augmentation import DataAugmentation\n",
    "from data_generator import DataGenerator\n",
    "from model import CustomModel as Model\n",
    "import numpy as np\n",
    "from utils import Utils\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from keras import backend as K\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Utils.get_hypes()\n",
    "\n",
    "my_file = Path(hyperparameters[\"dataset_dir\"] + \"labels.pkl\")\n",
    "if not my_file.is_file():\n",
    "    DataConverter(hyperparameters).convert_png_to_npy()\n",
    "    \n",
    "with open(hyperparameters[\"dataset_dir\"]+\"labels.pkl\", \"rb\") as file:\n",
    "    labels = pkl.load(file)\n",
    "\n",
    "pids = np.load(hyperparameters[\"dataset_dir\"] + \"pids.npy\")\n",
    "label_values = np.load(hyperparameters[\"dataset_dir\"] + \"labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization kfold or hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_split_index = 1\n",
    "val_split_index = 1\n",
    "#skf_tr_tst = StratifiedKFold(n_splits=hyperparameters[\"kfold\"][0],shuffle=hyperparameters[\"shuffle\"])\n",
    "skf_tr_val = StratifiedShuffleSplit(n_splits=hyperparameters[\"kfold\"][0], test_size=0.2, train_size=0.8)\n",
    "values = np.array(list(labels.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data, augmentation, model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 23:10:12.742325  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0922 23:10:12.742325  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0922 23:10:12.757950  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0922 23:10:12.773574  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0922 23:10:12.773574  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0922 23:10:15.441256  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0922 23:10:16.757139  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0922 23:10:16.819638  1220 deprecation.py:506] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0922 23:10:17.054013  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv0 (Conv2D)               (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv_bn0 (BatchNormalization (None, 150, 150, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv0_out (Activation)       (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv_bn1 (BatchNormalization (None, 75, 75, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv1_out (Activation)       (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv_bn2 (BatchNormalization (None, 75, 75, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2_out (Activation)       (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 38, 38, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv_bn3 (BatchNormalization (None, 38, 38, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv3_out (Activation)       (None, 38, 38, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 19, 19, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv_bn4 (BatchNormalization (None, 19, 19, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv4_out (Activation)       (None, 19, 19, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv_bn5 (BatchNormalization (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv5_out (Activation)       (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "AvgPooling2D (AveragePooling (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_layer (Flatten)      (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "fc0 (Dense)                  (None, 1024)              13108224  \n",
      "_________________________________________________________________\n",
      "drop0 (Dropout)              (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc_bn0 (BatchNormalization)  (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "softmax_layer (Dense)        (None, 8)                 8200      \n",
      "=================================================================\n",
      "Total params: 14,860,616\n",
      "Trainable params: 14,856,264\n",
      "Non-trainable params: 4,352\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 23:10:18.562630  1220 deprecation.py:323] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0922 23:10:22.680984  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0922 23:10:22.680984  1220 deprecation_wrapper.py:119] From C:\\Users\\Lefteris\\Anaconda3\\envs\\deep_learning\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "133/133 [==============================] - 26s 192ms/step - loss: 1.8085 - acc: 0.6160 - val_loss: 2.2239 - val_acc: 0.5414\n",
      "Epoch 2/2000\n",
      "133/133 [==============================] - 21s 160ms/step - loss: 1.4435 - acc: 0.7556 - val_loss: 1.9042 - val_acc: 0.5838\n",
      "Epoch 3/2000\n",
      "133/133 [==============================] - 21s 160ms/step - loss: 1.3096 - acc: 0.8073 - val_loss: 1.8790 - val_acc: 0.6434\n",
      "Epoch 4/2000\n",
      "133/133 [==============================] - 21s 161ms/step - loss: 1.1774 - acc: 0.8687 - val_loss: 1.3014 - val_acc: 0.8081\n",
      "Epoch 5/2000\n",
      "133/133 [==============================] - 22s 163ms/step - loss: 1.0948 - acc: 0.9035 - val_loss: 1.4678 - val_acc: 0.7071\n",
      "Epoch 6/2000\n",
      "133/133 [==============================] - 21s 161ms/step - loss: 1.0173 - acc: 0.9368 - val_loss: 1.3695 - val_acc: 0.7444\n",
      "Epoch 7/2000\n",
      "133/133 [==============================] - 21s 160ms/step - loss: 0.9322 - acc: 0.9659 - val_loss: 1.1767 - val_acc: 0.8667\n",
      "Epoch 8/2000\n",
      "133/133 [==============================] - 22s 166ms/step - loss: 0.8848 - acc: 0.9779 - val_loss: 1.1575 - val_acc: 0.8636\n",
      "Epoch 9/2000\n",
      "133/133 [==============================] - 22s 162ms/step - loss: 0.8374 - acc: 0.9890 - val_loss: 1.2189 - val_acc: 0.8253\n",
      "Epoch 10/2000\n",
      "133/133 [==============================] - 22s 164ms/step - loss: 0.7965 - acc: 0.9930 - val_loss: 1.2227 - val_acc: 0.8343\n",
      "Epoch 11/2000\n",
      "133/133 [==============================] - 22s 162ms/step - loss: 0.7762 - acc: 0.9937 - val_loss: 1.2358 - val_acc: 0.8101\n",
      "Epoch 12/2000\n",
      "133/133 [==============================] - 21s 161ms/step - loss: 0.7548 - acc: 0.9952 - val_loss: 1.1884 - val_acc: 0.8333\n"
     ]
    }
   ],
   "source": [
    "#for trval_index, tst_index in skf_tr_tst.split(pids,values):\n",
    "convergence_pids = pids#[trval_index]\n",
    "convergence_labels = values#[trval_index]\n",
    "for tr_index, val_index in skf_tr_val.split(convergence_pids,convergence_labels):\n",
    "    K.clear_session()\n",
    "    \n",
    "    #network version\n",
    "    version = str(tst_split_index)+\".\"+str(val_split_index)\n",
    "    hyperparameters[\"version\"] = \"network_version\"+version+\"\\\\\"\n",
    "    \n",
    "    #make dirs        \n",
    "    Utils.make_dirs(version,hyperparameters)        \n",
    "    \n",
    "    #save patient id per network version\n",
    "    Utils.save_skf_pids(version,convergence_labels[tr_index],convergence_pids[val_index],convergence_pids[val_index],hyperparameters)#pids[tst_index],hyperparameters)        \n",
    "    \n",
    "    #offline data augmentation\n",
    "    if hyperparameters[\"offline_augmentation\"] == True and hyperparameters[\"data_augmentation\"] == True:\n",
    "        training_set, training_labels = DataConverter(hyperparameters).load_npy_from_hdd(convergence_pids[tr_index], labels)\n",
    "        training_set, training_labels = DataAugmentation.apply_augmentation(training_set, training_labels,hyperparameters)\n",
    "        pids_tr, labels_tr=DataConverter(hyperparameters).save_augmented_samples(training_set, training_labels,convergence_pids[tr_index],convergence_labels[tr_index])\n",
    "    else:\n",
    "        pids_tr = convergence_pids[tr_index]\n",
    "        labels_tr = labels\n",
    "            \n",
    "    # Generators\n",
    "    training_generator = DataGenerator(pids_tr, labels_tr, hyperparameters, training=True)\n",
    "    validation_generator = DataGenerator(convergence_pids[val_index], labels, hyperparameters, training=False)\n",
    "#    testing_generator = DataGenerator(pids[tst_index], labels, hyperparameters, training=False)\n",
    "    \n",
    "    #create network\n",
    "    cnn = Model.get_model(hyperparameters)\n",
    "    \n",
    "    #fit network\n",
    "    cnn = Model.train_model(cnn,hyperparameters,training_generator,validation_generator)\n",
    "    Utils.save_hypes(hyperparameters[\"chkp_dir\"]+hyperparameters[\"version\"], \"hypes\"+version, hyperparameters)\n",
    "    \n",
    "    #Update version indeces\n",
    "    val_split_index+=1\n",
    "val_split_index=1\n",
    "tst_split_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
